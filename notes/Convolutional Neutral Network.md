# Convolution Neutral Network

### 一、神经网络

在学习卷积神经网络之前先了解一下神经网络。

#### 1.1 神经网络分析

<img src=".\img\两层神经网络.jpg" alt="两层神经网络" style="zoom: 80%;" />

##### 输入层

在例子中输入值是坐标值，是一个包含两个元素的一维数组。可以理解为输入层为输入一个矩阵，图中的就是1×2矩阵，输入一张32×32像素的灰色图像就是输入32×32的矩阵。

##### 从输入层到隐藏层

从输入层到隐藏层依靠权重矩阵W1和偏置b1，其实就是进行矩阵运算：H = X × W1 + b1。包括隐藏层之间传输和从隐藏层到输出层也是一样的计算方法，只不过用了不同的权重矩阵和偏置。

##### 激活层

激活层需要在每个隐藏层计算之后，不然该层的线性计算就是没有意义的。激活层，或是激励层在之后卷积神经网络中展开。

##### 输出层

数据到达输出层之后其实我们已经拿到了一个结果，比如图中在softmax之前输出的值可能会是(3,1,0.1,0.5)这样的1×4矩阵，我们已经可以找打这里面的最大值3了，但是这个结果不够直观，最好是直接输出属于某个类的概率，比如输出(90%,5%,2%,3%)这样的结果，因此我们还需要进行输出正规化。

简单来说分为三步：（1）以e为底对所有元素求指数幂；（2）将所有指数幂求和；（3）分别将这些指数幂与该和做商。其实就是每一项指数幂求和，然后概率就是单项指数幂占总数的比例。使用这种计算公式进行结果正规化处理就称为Softmax。

##### 衡量输出好坏

经过Softmax之后就得到了一个结果，这时候需要对这个输出结果的好坏程度进行一个“量化”。比较典型的一种量化方法是使用**交叉熵损失( Cross Entropy Loss )**。这个损失的想法很简单，就是求对数的负数，比如对于0.9，结果就是-log0.9 = 0.046，概率越接近100%，交叉熵损失就越接近0.

##### 反向传播与参数优化

之前说过神经网络的传播都是形如Y=WX+b的矩阵运算，为了给矩阵运算加入非线性，需要在隐藏层中加入激活层。在计算出交叉熵损失之后就要反向传播来进行参数优化，使结果最为准确，这里的参数指的就是W和b，毕竟只有这两个是可变的。反向传播的原理很简单，就是不断地微调这些可变参数，然后再次计算损失值，不断迭代，使损失值越来越小，直到我们满意为止。

#### 1.2 反向传播

反向传播算法本质上是梯度下降算法，详见[神经网络反向传播算法](https://zhuanlan.zhihu.com/p/25609953)。挺详细的，但还没有仔细看。

### 二、卷积神经层级架构

#### 2.1 数据输入层

数据输入层主要做的是对原始数据进行预处理。

* 去均值：把输入数据各个维度都中心化为0，其目的就是把样本的中心拉回到坐标系原点上。
* 归一化：幅度归一化到同样的范围，即减少各维度数据取值范围的差异而带来的干扰，一般归一至0到1的范围
* PCA/白化：PCA降维；白化是对数据各个特征轴上的幅度归一化

#### 2.2 卷积计算层

卷积计算层是卷积神经网络最重要的一个层次，有两个关键操作：**局部关联**和**窗口滑动**

* 深度：每个神经元看作一个滤波器 filter，有多少个神经元，深度depth就是多少
* 步长：步长stride是窗口滑动一次的长度
* 填充值：有些时候滑动窗口无法遍历所有的像素，这时候就需要在原先矩阵填充几层，填进去的就是填充值padding，通常使用zero-padding也就是使用0来填充

##### 卷积的计算

卷积操作中，一个 3×3×3 的子节点矩阵和一个 3×3×3 的 filter 对应元素相乘，得到的是一个 3×3×3 的矩阵，此时将该矩阵所有元素求和，得到一个 1×1×1 的矩阵，将其再加上 filter 的 bias，经过激活函数得到最后的结果，将最后的结果填入到对应的输出矩阵中。

<img src=".\img\卷积计算01.jpg" alt="卷积计算01" style="zoom:80%;" />

这里的蓝色矩阵就是输入的图像，粉色矩阵就是卷积层的神经元，这里表示了有两个神经元（w0,w1）。绿色矩阵就是经过卷积运算后的输出矩阵，这里的步长设置为2。该实例中输入矩阵为7×7×3，滤波器为3×3×3，而输出矩阵为3×3×2，滑动窗口每移动一个步长，**计算三层的窗口矩阵与滤波器矩阵对应位相乘再相加，最后加上偏置值得到一个输出**（例如图中就是 0 + (-2-2) + 0 + 1 = -3 ）的计算，最终得到3×3的矩阵，因为有两个滤波器所以输出矩阵层数为2。

<img src="E:\李云昊\国科\computer-vision\notes\img\单层卷积网络.jpg" alt="单层卷积网络"  />

##### 参数共享机制

在卷积层中每个神经元连接数据窗的权重是固定的，每个神经元只关注一个特性。神经元就是图像处理中的滤波器，比如边缘检测专用的Sobel滤波器，即卷积层的每个滤波器都会有自己所关注一个图像特征，比如垂直边缘，水平边缘，颜色，纹理等等，这些所有神经元加起来就好比就是整张图像的特征提取器集合。

#### 2.3 ReLU激励层

把卷积层输出结果做非线性映射。CNN采用的激励函数(激活函数)一般是ReLU( The Rectified Linear Unit 修正线性单元 )，它的特点是收敛快，求梯度简单，但是比较脆弱。

神经网络中每一层进行矩阵运算之后得到结果，激活函数或者说是激励函数就是为了给该结果添加非线性用的，常见的激活函数有阶跃函数、Sigmoid函数和ReLU函数，基础CNN中通常使用ReLU。三种函数见下图所示：

<img src=".\img\激活函数.jpg" alt="三种激活函数" style="zoom:80%;" />

##### 必要性

线性代数有一个特性，**一系列线性方程的运算最终都可以用一个线性方程表示**。所以无论神经网络有多少层，多个式子联立之后都可以用一个线性方程表达，这样的话神经网络就失去了意义，因此需要激活层（ReLU比较脆弱的意思就是比较容易被破解）。

#### 2.4 池化层

池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。简而言之，**如果输入是图像的话，那么池化层的最主要作用就是压缩图像**。

##### 具体作用

* 特征不变性：图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。
* 特征降维：一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来。
* 在一定程度上防止过拟合，更方便优化。

##### 池化层方法

池化层常方法有 Max Pooling 和 Average Pooling，实际使用比较多的是Max Pooling。

<img src=".\img\max pooling.jpg" alt="Max Pooling" style="zoom:67%;" />

对于每个2 * 2的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个2 * 2窗口中最大的数是6，那么输出矩阵的第一个元素就是6，如此类推。同样的 Average Pooling 就是去平均值，不再展开。

*池化层一般不改变矩阵深度，只改变长和宽*

#### 2.5 全连接层

CNN中两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部，也就是跟传统的神经网络神经元的连接方式是一样的。经过多轮卷积层和池化层的处理后，在CNN的最后一般由1到2个全连接层来给出最后的分类结果。经过几轮卷积和池化操作，可以认为图像中的信息已经被抽象成了信息含量更高的特征。我们可以将卷积和池化看成自动图像提取的过程，在特征提取完成后，仍然**需要使用全连接层来完成分类任务**。

### 三、Notes

##### channel是什么？

* 最初输入的图片样本的 channels ，取决于图片类型，比如RGB类型就是3。
* 卷积操作完成后输出的 out_channels ，取决于卷积核的数量。此时的 out_channels 也会作为下一次卷积时的卷积核的 in_channels。
* 卷积核中的 in_channels ，刚刚2中已经说了，就是上一次卷积的 out_channels ，如果是第一次做卷积，就是1中样本图片的 channels 。

##### 神经网络与卷积神经网络

从广义上来说卷积神经网络也属于多级神经网络的一种，他在原来多级神经网络的基础上加入了特征学习的部分。具体操作就是在原来的全连接层钱买你加入了部分连接的卷积层和激活层，使完整的层级变为：**输入层 - 卷积层 - 激活层 - …… - 隐藏层 - 输出层**。或者可以直接理解为卷积神经网络是在神经网络分类之前先利用卷积进行特征提取，卷积不是结合在神经网络里面的！**~~不要理解为神经网络的隐藏层中进行卷积运算！~~**

### 四、Reference

* [神经网络15分钟入门！足够通俗易懂了吧](https://zhuanlan.zhihu.com/p/65472471)
* [一文让你理解什么是卷积神经网络  简书](https://www.jianshu.com/p/191d1e21f7ed)
* [卷积神经网络(CNN)  手记](https://www.imooc.com/article/68983)
* [吴恩达 卷积神经网络 CNN  简书](https://www.jianshu.com/p/da0db799a681)
