# Deep Residual Learning for Image Recognition

### 一、简介

更深的卷积网络层可以对目标检测很有帮助，但是也产生了一个新的问题：**训练一个更好的网络是否和堆叠更多的层一样简单？**解决这一问题的障碍便是困扰人们很久的梯度消失/梯度爆炸，这从一开始便阻碍了模型的收敛。**归一初始化（normalized initialization）和中间归一化（intermediate normalization）**在很大程度上解决了这一问题，它使得数十层的网络在反向传播的随机梯度下降（SGD）上能够收敛。

深层网络能够收敛时，一个**退化**问题又出现了：随着网络深度的增加，准确率达到饱和然后迅速退化。意外的是，这种退化并不是由过拟合造成的，并且在一个合理的深度模型中增加更多的层却导致了更高的错误率。

现存的对这个问题的解决方法是通过构建的解决方案：**恒等映射**（identity mapping）来构建增加的层，而其它层直接从浅层模型中复制而来（具体见[notes相关部分](#####恒等映射构建)）。原文提出了一种名为**深度残差学习框架**的方法来解决退化问题（是上述构建方法的一种实现）。在这种方法中我们让这些层来拟合**残差映射**（residual mapping），而不是让每一个堆叠的层直接来拟合所需的底层映射。换句话说，对于一个堆积层结构（几层堆积而成）当输入为 ![[公式]](https://www.zhihu.com/equation?tex=x) 时其学习到的特征记为 ![[公式]](https://www.zhihu.com/equation?tex=H%28x%29) ，现在我们希望其可以学习到残差 ![[公式]](https://www.zhihu.com/equation?tex=F%28x%29%3DH%28x%29-x) ，这样其实原始的学习特征是 ![[公式]](https://www.zhihu.com/equation?tex=F%28x%29%2Bx) 。

这种框架的实现靠的是shortcut，如下图所示：

![](.\img\残差映射shortcut.jpg)

Shortcut 连接就是跳过一个或者多个层。在原文的例子中，shortcut 连接只是简单的执行恒等映射，再将它们的输出和堆叠层的输出叠加在一起。这样子这两层（或者说一个building block）

### 二、相关工作

#### 2.1 残差描述

VLAD（Vector of Aggragate Locally Descriptor）是残差向量对应于字典进行编码的一种表达形式，Fisher Vector可以看做是VLAD 的一个概率版本。ResNet使用Fisher Vector 来描述残差。

VLAD的主要方法是通过聚类方法训练一个小的码本，对于每幅图像中的特征找到最近的**码本聚类中心**，随后**所有特征与聚类中心的差值**做累加，得到一个k×d的vlad矩阵，其中k是聚类中心个数，d是特征维数(如sift是128维),随后将该矩阵扩展为一个(k×d)维的向量，并对其L2归一化，所得到的向量即为VLAD。

### 三、深度残差学习

#### 3.1 残差学习

首先我们将 H(x) 看作一个由部分堆叠的层映射得到的结果，其中 x 是这些层的输入。假设多个非线性层能够逼近复杂的函数，那么这些层也能够逼近复杂的残差函数。我们让这些不去近似得到输出 H(x)，而是让他们去近似残差函数 F(x) := H(x) - x，这时候原函数就变成了 F(x) + x（尽管这两个形式应该都能够逼近所需的函数，但是学习的难易程度并不相同）。

#### 3.2 Identity Mapping by Shortcuts

shortcut的实现形式基本上就如[简介](###一、简介)中所示。不过图中所示的只能用在 H(x) 和 x 是相同的维度的，如果在中间的非线性卷积层中通道数发生了变化，那么就需要在shortcut连接中添加一个线性映射来匹配两者的维度。这种形式对于全连接层和卷积层都是可行的。

#### 3.3 网络架构

<img src=".\img\ResNet网络架构.jpg"  />

##### Plain Network

 朴素网络架构和VGG模型类似，由一些3×3的卷积层构成。不同于VGG的是ResNet在进行channels翻倍特征图尺寸减半的步骤时候不是使用max pooling，而是使用3×3卷积核且步长为2的卷积层。卷积层结束之后跟着的是是一个average pooling，一个1000输出通道的全连接层和一个softmax层。注意这个网络虽然层数很多，但是实际上他的过滤器数量要少于VGG-19（主要是因为过滤器最后有两个4096通道的），他的计算复杂度也要小于VGG。

##### Residual Network

从图中可以很清晰地看到残差网络就是在朴素网络的基础上添加了许多shortcut连接。当前后维度一致时（如实线连接），shortcut连接可以直接使用，而当前后不一致时（如虚线连接），有两种方式：一是用0来填充缺失的维度（identity shortcut），二是之前说的加一个线性映射（projection shortcut）。无论使用哪种方法，在前后维度不一致的情况下shortcut的stride都是2。

##### ResNet-50和ResNet-101

图中展示的是34层的ResNet，但ResNet还有很多其他的类型，其中比较典型的是ResNet-50和ResNet101,其中50层及其之后的网络出现了1×1的卷积层，这种方法称为bottleneck技术（具体见Notes[相关部分](#####bottleneck)）他们的样式如下图所示：

<img src=".\img\不同ResNet.jpg" style="zoom:80%;" />

#### 3.4 实施

传入ResNet的图片首先被resize到最短边为[256, 480]中的一个随机值，然后从这个图像或者它的水平翻转图像中随机剪裁出一个224×224的图像并减去他们的像素平均值，最后还要进行一个 standard color augmentation（标准颜色增强）。传入卷积神经网络之后，在每一个卷积层计算结束，激活层计算之前还要进行 batch normalization。

训练时使用SGD，并使用256张的mini-batch。学习率初始时设为0.1，每次遇到错误率平台期就除以10，模型训练最多迭代60万次。同时使用0.0001的权重衰减和0.9的冲量。

测试时，对于比较测试选用了 **standard 10-crop test**，对于最佳表现测试使用了全卷积层的模型（思路和VGG的一样），然后平均不同尺寸图像的得分（测试图像传入时最小边适应{224,256,384,480,640}）。

standard 10-crop test 是指在传入图像及其水平反转图像（大小变化之后）的4个corner和center各截取224×224的图像，总共十张进行测试。

### Notes

##### 恒等映射构建

对任意集合A，如果映射 f:A→A 定义为f(a)=a，即规定A中每个元素a与自身对应，则称f为A上的**恒等映射 **(identical [identity] mapping)。*所以说这是个数学概念！*

这种方法大致是这样的：现在你有一个浅层网络，你想通过向上堆积新层来建立深层网络，一个极端情况是这些增加的层什么也不学习，仅仅复制浅层网络的特征，即这样新层是恒等映射，这样至少不会发生退化。

##### bottleneck

Bottleneck layer又称之为瓶颈层，使用的是1*1的卷积神经网络。使用1×1的卷积网络的一大好处就是可以大幅减少计算量。以ResNet-50为例，它的每个building block都是由三层组成（1×1 + 3×3 + 1×1），第一个1×1的卷积层目的是降低参数为维度，也就是通道数，然后传入3×3卷积层进行正常的学习，最后一个1×1卷积层则再将维度恢复，也就是说和原来相比，在3×3卷积层前后套上1×1卷积层之后能够有效降低3×3卷积层的参数数量，从而降低了计算复杂度。

##### maxout

maxout是一个激活函数，在Maxout网络中，其隐含层节点的输出表达式为： ![[公式]](https://www.zhihu.com/equation?tex=%7Bf_i%7D%28x%29+%3D+%5Cmathop+%7B%5Cmax+%7D%5Climits_%7Bj+%5Cin+%5B1%2Ck%5D%7D+%7Bz_%7Bij%7D%7D)，其中 ![[公式]](https://www.zhihu.com/equation?tex=%7Bz_%7Bij%7D%7D+%3D+%7Bx%5ET%7D%7BW_%7B...ij%7D%7D+%2B+%7Bb_%7Bij%7D%7D%2CW+%5Cin+%7BR%5E%7Bd+%5Ctimes+m+%5Ctimes+d%7D%7D)。假设 w 是 2 维，那么有 ![[公式]](https://www.zhihu.com/equation?tex=f%28x%29+%3D+%5Cmax+%28%7Bw_1%7D%5ETx+%2B+%7Bb_1%7D%2C%7Bw_2%7D%5ETx+%2B+%7Bb_2%7D%29)。Maxout的拟合能力是非常强的，它可以拟合任意的的凸函数。

